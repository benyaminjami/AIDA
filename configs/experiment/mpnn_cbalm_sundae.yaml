# @package _global_

# to execute this experiment run:

defaults:
  - /data: sabdab
  - /trainer: default

# name of the run determines folder name in logs
task_name: "sundae-train"

data:
  alphabet:
    encoder:
      name: mpnn
      featurizer: mpnn
    decoder:
      name: esm
      featurizer: balm
  
model:
  _target_: src.models.mpnn_cbalm.MPNNcBALM
  cfg:
    encoder:
      pretrained_path: ${paths.data_dir}/pretrained_models/pretrained-mpnn
    decoder:
      model: balm
      pretrained_path: ${paths.data_dir}/pretrained_models/pretrained-BALM
      balm_config:
        adapter_layer_indices: 1 # Can be list of indices or an int. In case of int the adapter will be added every n layers.
      adapter_config:
        encoder_hidden_size: 256
        position_embedding_type: null
        is_cross_attention: True
      


# task
task:
  _target_: src.tasks.SUNDAE.SUNDAE
  _recursive_: False
  alphabet: ${data.alphabet}
  
  # model
  model: ${model}

  learning:
    unroll_steps: 1 # number of unroll steps
    noise: selected_guided_sundae
    noise_mask: cdr_weights
    mask_loss: True
    avg_unroll_loss: True
    
  generator:    
    max_iter: 5
    strategy: denoise  
    tasks:
      h3:
        name: h3
        noise: selected_random
        mask: h3_mask
      cdr:
        name: cdr
        noise: selected_random
        mask: cdr_weights
      full:
        name: full
        noise: full_random
    temperature: 0
    
  criterion:
    _target_: src.utils.criterion.CrossEntropyLoss
    label_smoothing: 0.0
    ignore_index: 1 

  optimizer:
    type: adamw
    lr: ${train.lr}
    betas:
      - 0.9
      - 0.98
    weight_decay: 0.0001
  lr_scheduler:
    type: inversesqrt
    lr: ${train.lr}
    warmup_steps: 1000
    model_size: 10000
    warmup_init_lr: 1e-07

# training related
train:
  seed: 42
  lr: 1e-4
  monitor: "val/full_acc"
  mode: "max"

trainer:
  min_epochs: 10
  max_epochs: 10000
  gradient_clip_val: 0.0
  # val_check_interval: 10
  num_sanity_val_steps: 1
  reload_dataloaders_every_n_epochs: 1
  use_distributed_sampler: false
  max_steps: 200_000
  precision: 16


# callbacks:
#   mpnn_finetune:
#     _target_: pytorch_lightning.callbacks.BackboneFinetuning
#     unfreeze_backbone_at_epoch: 10
#     # lambda_func: 1e-4
#     backbone_initial_ratio_lr: 0.1
#     backbone_initial_lr: null
#     should_align: true
#     initial_denom_lr: 10.0
#     train_bn: true
#     verbose: true
#     rounding: 12



# callbacks:
#   balm_finetune:
#     _target_: src.utils.callbacks.CustomBackboneFinetuning
#     unfreeze_backbone_at_epoch: [10]
#     lambda_func: 1e-4
#     backbone_initial_ratio_lr: 0.1
#     backbone_initial_lr: null
#     should_align: true
#     initial_denom_lr: 10.0
#     train_bn: true
#     verbose: true
#     rounding: 12
#     backbone_module_paths: [model.encoder]
    # excluded_names: ['crossattention', 'chain']

  # mpnn_finetune:
  #   _target_: src.utils.callbacks.CustomBackboneFinetuning
  #   unfreeze_backbone_at_epoch: 10
  #   lambda_func: 1e-4
  #   backbone_initial_ratio_lr: 0.1
  #   backbone_initial_lr: null
  #   should_align: true
  #   initial_denom_lr: 10.0
  #   train_bn: true
  #   verbose: true
  #   rounding: 12
  #   backbone_module_path: encoder